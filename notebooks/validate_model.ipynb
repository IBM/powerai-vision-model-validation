{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PowerAI Vision Model Validation\n",
    "\n",
    "A PowerAI Vision model is built with testing and training within the PowerAI Vision UI. Additional manual testing can be done from within the UI.\n",
    "This notebook demonstrates a variety of metrics that can be collected using Python code and the API endpoint of a deployed model.  The test images\n",
    "are read from local directories and the results are shown in this notebook and also exported to CSV files.\n",
    "\n",
    "![](../doc/source/images/confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Setup\n",
    "\n",
    "Edit the following cell to provide the API endpoint of your deployed PowerAI Vision model in the URL variable.\n",
    "Customize additional constants (e.g. set your input directory) as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy/paste API endpoint from your PowerAI Vision deployed model.\n",
    "URL = 'https://url/guid'\n",
    "\n",
    "# Provide an input directory. Its subdirectories contain test images.\n",
    "INPUT_DIR = '../data/test'  # Use subdir names as ground truth category names or category_map\n",
    "\n",
    "OUTPUT_PREFIX = 'result'  # For output CSV file(s)\n",
    "\n",
    "# WARNING: If False, for convenience, we are not validating the certificate when using the PowerAI API endpoint.\n",
    "VERIFY_CERT = False\n",
    "\n",
    "# Map directory name of test data to a category name. If not mapped, the directory name will be used.\n",
    "category_map = {'directory_name': 'map-to-category-name',\n",
    "                'Aircraft': 'aircraft',\n",
    "                'Watercraft': 'watercraft',\n",
    "                'Land vehicle': 'land vehicle'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user pandas_ml==0.6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import collections\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import pandas\n",
    "from pandas_ml import ConfusionMatrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use logger to control debug output levels.\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger(\"logger\")\n",
    "# Set desired logging to ERROR, INFO, WARNING, or DEBUG.\n",
    "logger.setLevel(\"ERROR\")\n",
    "logger.debug(\"FYI -- Debug logging is enabled.\")\n",
    "logger.info(\"FYI -- Info logging is enabled.\")\n",
    "logger.error(\"FYI -- Error logging is enabled.\")\n",
    "requests.packages.urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expression to only classify JPG or PNG images.\n",
    "file_matcher = re.compile(\"jpg$|jpeg$|png$\", re.IGNORECASE)\n",
    "\n",
    "#------------------------------------\n",
    "# classify files in directory and save results in dict\n",
    "# returns dict upon completion\n",
    "def classifyFiles(directory, normalize):\n",
    "    results = {}\n",
    "    totalFiles = 0\n",
    "    skippedFiles = 0\n",
    "    processedFiles = 0\n",
    "    classifiedFiles = 0\n",
    "    unclassifiedFiles = 0\n",
    "    problemFiles = 0\n",
    "\n",
    "    # To help track millisecond time for each inference\n",
    "    current_milli_time = lambda: int(round(time.time() * 1000))\n",
    "\n",
    "    logger.info(\"Classifying directory: {}\".format(directory))\n",
    "    for filename in os.listdir(directory):\n",
    "        totalFiles += 1\n",
    "        if file_matcher.search(filename):\n",
    "            logger.debug(\"classifying {}\".format(filename))\n",
    "\n",
    "            processedFiles += 1\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            files = {'files': open(filepath, 'rb')}\n",
    "\n",
    "            startMilli = current_milli_time()\n",
    "            rsp = requests.post(URL, verify=False, auth=None, files=files)\n",
    "            stopMilli = current_milli_time()\n",
    "            duration = stopMilli - startMilli\n",
    "            logger.debug(\"classified {} in {} millis\".format(filename, duration))\n",
    "            if rspOk(rsp):\n",
    "                result = rsp.json()\n",
    "                logger.debug(\"json={}\".format(json.dumps(result)))\n",
    "                if \"classified\" in result:\n",
    "                    classification = result[\"classified\"]\n",
    "                    logger.debug(\"classification = {}\".format(json.dumps(classification)))\n",
    "                    for classname in classification.keys():\n",
    "                        confidence = classification[classname]\n",
    "\n",
    "                        if classname == \"negative\":\n",
    "                            unclassifiedFiles += 1\n",
    "                            if normalize:\n",
    "                                # translate 'negative' to 'non-classified' for better clarity\n",
    "                                classname = \"unclassified\"\n",
    "                                confidence = 0\n",
    "                        else:\n",
    "                            classifiedFiles += 1\n",
    "\n",
    "                        logger.debug(\"Saving result {}, {}, {}\".format(filename, classname, confidence))\n",
    "                        results[filename] = {\"filename\": filename,\n",
    "                                             \"classification\": classname,\n",
    "                                             \"confidence\": confidence,\n",
    "                                             \"duration\": duration}\n",
    "                else:\n",
    "                    problemFiles += 1\n",
    "                    results[filename] = {\"filename\": filename,\n",
    "                                         \"classification\": \"None\",\n",
    "                                         \"confidence\": \"\",\n",
    "                                         \"duration\": duration}\n",
    "                    logger.warning(\"No classification for {}\".format(filename))\n",
    "            else:\n",
    "                logger.error(\"ERROR\")\n",
    "                problemFiles += 1\n",
    "                logger.error(\"Error result from server for {}\".format(filename))\n",
    "        else:\n",
    "            skippedFiles += 1\n",
    "            logger.info(\"skipping non-image file {}\".format(filename))\n",
    "\n",
    "    logger.info(\"TotalFiles: {}, ProcessedFiles: {}, classifiedFiles: {}, unclassifiedFiles: {}, skippedFiles: {}, problemFiles: {}\".\n",
    "                 format(totalFiles, processedFiles, classifiedFiles, unclassifiedFiles, skippedFiles, problemFiles))\n",
    "    return results\n",
    "\n",
    "# ------------------------------------\n",
    "# Checks if result from Vision API succeeded\n",
    "# (Current API returns failure indication in the JSON body)\n",
    "def rspOk(rsp):\n",
    "    logger.debug(\"status_code: {}, OK={}.\".format(rsp.status_code, rsp.ok))\n",
    "\n",
    "    if rsp.ok:\n",
    "        try:\n",
    "            jsonBody = rsp.json()\n",
    "            if (\"result\" in jsonBody) and (jsonBody[\"result\"] == \"fail\"):\n",
    "                result = False\n",
    "                logger.debug(json.dumps(jsonBody, indent=2))\n",
    "            else:\n",
    "                result = True\n",
    "        except ValueError:\n",
    "            result = True\n",
    "            logger.error(\"good status_code, but no data\")\n",
    "    else:\n",
    "        result = False\n",
    "\n",
    "    return result\n",
    "\n",
    "# Use dirname to get ground truth category name from map (default to dirname)\n",
    "def get_ground_truth(dirname):\n",
    "    return category_map.get(dirname, dirname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "\n",
    "The main logic starts here and continues below.\n",
    "\n",
    "* Loop over the `INPUT_DIR` sub-directories.\n",
    "* Call the inference function on each image file.\n",
    "* Use the built truth and predicted lists for the rest (below).\n",
    "\n",
    "  * \"truth\" lists the classification we want for the image.\n",
    "  * \"predicted\" lists the classification we got from the inference call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# MAIN\n",
    "#\n",
    "\n",
    "truth = []\n",
    "predicted = []\n",
    "\n",
    "for dirname in os.listdir(INPUT_DIR):\n",
    "    if not os.path.isdir(os.path.join(INPUT_DIR, dirname)):\n",
    "        continue\n",
    "    \n",
    "    result_dict = classifyFiles(os.path.join(INPUT_DIR, dirname), False)\n",
    "    logger.debug(result_dict)\n",
    "    \n",
    "    for k,v in result_dict.items():\n",
    "        truth.append(get_ground_truth(dirname))  # Ground truth determined from dirname\n",
    "        predicted.append(v['classification'])  # The predicted classification\n",
    "        \n",
    "confusion_matrix = ConfusionMatrix(truth, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the confusion matrix\n",
    "\n",
    "Using matplotlib and seaborn, we can graphically show the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix.plot(backend='seaborn', annot=True, linewidth=5, cmap='Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the normalized confusion matrix\n",
    "\n",
    "Setting `normalized=True` gives us values from 0 to 1. This may be a better representation when the wights are uneven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix.plot(normalized=True, backend='seaborn', annot=True, linewidth=5, cmap='Greens');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The handiest way to see the stats is with print_stats().\n",
    "# But since we're coding for CSV output we have prettier ways\n",
    "# to show the data below.\n",
    "\n",
    "# confusion_matrix.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the classification metrics by class and write them to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = confusion_matrix.stats()\n",
    "\n",
    "class_csv = OUTPUT_PREFIX + \"_class.csv\"\n",
    "logger.info(\"Writing stats by class to: \" + class_csv)\n",
    "\n",
    "with open(class_csv, \"w\") as outfile:\n",
    "    csvwriter = csv.writer(outfile)\n",
    "\n",
    "    first = True\n",
    "    for classification, v in stats['class'].items():\n",
    "        header = ['class']\n",
    "        row = [classification]\n",
    "        for x, y in v.items():\n",
    "            header.append(x)\n",
    "            row.append(y)\n",
    "        if first:\n",
    "            csvwriter.writerow(header)\n",
    "            first = False\n",
    "        csvwriter.writerow(row)\n",
    "\n",
    "# Read and show the CSV file.\n",
    "df = pandas.read_csv(class_csv)\n",
    "df.style.hide_index()  # Preview here. To see the whole thing open the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T  # Same data as above, but transposed for a better view of the metrics (perhaps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the overall metrics\n",
    "\n",
    "The TP, TN, FP, and FN can be summed using the confusion matrix data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total TP, TN, FP, FN.\n",
    "tp_tn_fp_fn = df.agg({'TP: True Positive': ['sum'],\n",
    "                      'TN: True Negative': ['sum'],\n",
    "                      'FP: False Positive': ['sum'],\n",
    "                      'FN: False Negative': ['sum']})\n",
    "tp_tn_fp_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use count to calculate the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total class count.\n",
    "class_count = df.agg({'class': ['count']})\n",
    "class_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use max to calculate the number of images classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total population.\n",
    "population = df.agg({'Population': ['max']})\n",
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas-ml provides some overall metrics in a classification report\n",
    "\n",
    "This is the easy way, but it only offers weighted metrics and does not include MCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted classification report\n",
    "classification_report = confusion_matrix.classification_report\n",
    "classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn.metrics directly to get more metrics\n",
    "\n",
    "This gives us MCC and accuracy and also allows us to specify \"macro\" or \"weighted\" where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(truth, predicted)\n",
    "\n",
    "average = 'weighted'  # Use 'macro' or 'weighted'\n",
    "precision = precision_score(truth, predicted, average=average)\n",
    "recall = recall_score(truth, predicted, average=average)\n",
    "f1 = f1_score(truth, predicted, average=average)\n",
    "mcc = matthews_corrcoef(truth, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the overall metrics and write them to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_csv = OUTPUT_PREFIX + \"_summary.csv\"\n",
    "logger.info(\"Writing stats summary to: \" + summary_csv)\n",
    "\n",
    "# Pull some numbers out of the dataframes shown earlier\n",
    "images = population.iat[-1, -1]\n",
    "classes = class_count.iat[-1, -1]\n",
    "tp = tp_tn_fp_fn['TP: True Positive']['sum']\n",
    "tn = tp_tn_fp_fn['TN: True Negative']['sum']\n",
    "fp = tp_tn_fp_fn['FP: False Positive']['sum']\n",
    "fn = tp_tn_fp_fn['FN: False Negative']['sum']\n",
    "\n",
    "# Combine the metrics in a CSV with a header\n",
    "with open(summary_csv, \"w\") as outfile:\n",
    "    csvwriter = csv.writer(outfile)\n",
    "\n",
    "    first = True\n",
    "    header = ['# of Classes', '# of Images', 'TP', 'TN', 'FP', 'FN', 'Precision', 'Recall', 'Accuracy', 'F1', 'MCC']\n",
    "    row = [classes, images, tp, tn, fp, fn, precision, recall, accuracy, f1, mcc]\n",
    "    csvwriter.writerow(header)\n",
    "    csvwriter.writerow(row)\n",
    "    \n",
    "# Read and show the CSV file.\n",
    "summary = pandas.read_csv(summary_csv) # Preview the CSV file here\n",
    "summary.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1 color=gray>\n",
    "<hr>\n",
    "&copy; Copyright 2003,2016,2019 IBM Corp. All Rights Reserved.\n",
    "<p>\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file\n",
    "except in compliance with the License. You may obtain a copy of the License at\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the\n",
    "License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "express or implied. See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "</font></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
